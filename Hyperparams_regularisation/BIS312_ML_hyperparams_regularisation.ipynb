{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd63f7f-e528-4cf8-ad36-c3591b0f549a",
   "metadata": {},
   "source": [
    "# Hyperparameters, hyperparameter search and regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ddfc0-64c8-4957-8354-f2fbbe16f8bd",
   "metadata": {},
   "source": [
    "Last week we fit a very basic Logistic Regression model to our data in order to predict whether a patients tumour was benign or malignant on the basis of some simple physical measurements. \n",
    "\n",
    "We saw that even this this very simple model, that some people wouldn't even call machine learning, let alone AI, we did pretty good job, at least on the training data. However, by the time we looked at the test data, we saw that we weren't doing well enough for clinical settings. \n",
    "\n",
    "Today we will look at more complex models, known as Lasso, Ridge and ElastiNet. These models are similar in mathematical structure of Logistic Regression, but include methods for reducing the problem of overfitting. \n",
    "\n",
    "Moreoever, they will introduce us to an important new concept - _Hyperparameters_.\n",
    "\n",
    "By the end of this excercise you should:\n",
    "* Know what regularisation is and how it can reduce overfitting.\n",
    "* Be able to identify hyperparameters of a model.\n",
    "* Use random hyperparameter search with cross validation to select the best hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68344fbd-b560-4a87-b581-66b107392f24",
   "metadata": {},
   "source": [
    "## Regularised Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6b99d-3d58-4d2e-ba12-ff64444aac62",
   "metadata": {},
   "source": [
    "    \"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk\"\n",
    "    --- John Von Neumann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eab30f-6548-439d-b638-4c72efd93148",
   "metadata": {},
   "source": [
    "We ran into the problem of overfitting last week, when our performance on the training data was better than our preformance on the test data. Over fitting happens when your model fits too closely to the actual examples you give it, and doesn't generalise to other examples. \n",
    "\n",
    "It can be visualised thus:\n",
    "\n",
    "Lets say you are trying to draw a line between the red dots and blue dots (this is basically what classification based ML does, but in many dimensions). Your line to just be a straight line - it will not be very good at distinguishing the red from blue dots. However, if it is to wiggly, it make special cases for indevidual examples, and this does not generalise well.\n",
    "\n",
    "![](overfitting.png)\n",
    "\n",
    "The more parameters you have in your model, the easier it is to make the line wiggly. \n",
    "\n",
    "Regularisation is a way of reducing the number of parameters your model actaully uses, so that it only uses the ones it really needs to. Its a mathematical formulation of occums razor that we should prefer the simpliest possible explaination of our data. \n",
    "\n",
    "### The loss function\n",
    "\n",
    "Remember we said that ML models try to find the value of their parameters, so that when you plug the Xs into the equation, the Ys that come out are as close to the real Ys as possible? Well the models need a way to measure how close they are. This is call a models \"loss function\". The loss function measure how far away you are from the right answer. Lets not worry about how this is calculated for the moment, but just accept its existance (if you are interested, you can look up cross-entropy loss). \n",
    "\n",
    "### Ridge Regession\n",
    "\n",
    "Ridge regression simply adds up all the paramter values, and then adds them to the loss. \n",
    "\n",
    "$$Loss = L(y, \\bar{y}) + \\frac{1}{C}\\sum_j{{\\beta_j}^2}$$\n",
    "\n",
    "where L is the normal loss from the Logistic Regession model for the current set of parameters, $y$ is the real answer, $\\bar{y}$ is the models guess at $y$, $\\beta$ are the current value of the parameters, and $\\lambda$ is a tuning parameter, or \"Hyperparameter\", that says how big the penalty for using big values of $\\beta$ should be. \n",
    "\n",
    "If the model fits pefection (that is $L(y,\\bar{y}) = 0$), but to do that it needs to use lots of really big values for $\\beta$, then the loss is still high, and the machine learning algorithm will see this as a bad choice of $\\beta$s. \n",
    "\n",
    "Conversely, the model could make a prediction using very small values for $\\beta$, but if that means its not very accurate, then $L(y,\\bar{y})$ will be big, and the machine learning algorithm will also see this as a bad choice of $\\beta$s. \n",
    "\n",
    "Lets see how this works. We'll start by loading up the same dataset as before, and running the the same fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729baaa-a2d6-47ec-ab8b-9085a70f6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Aquire the data\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, Y = breast_cancer[\"data\"], breast_cancer[\"target\"]\n",
    "class_labels = breast_cancer[\"target_names\"]\n",
    "feature_labels = breast_cancer[\"feature_names\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, shuffle=True, random_state=2)\n",
    "\n",
    "# Note that I'm going to take only the first 100 examples, to make the task harder for the models\n",
    "X_train, Y_train = X_train[:100], Y_train[:100]\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12512d-df69-4a8f-aa77-4ff49b89f1c0",
   "metadata": {},
   "source": [
    "Now that we have set up the data, we can continue on to fit both standard Logistic Regression models, and a model with Ridge Regularisatoin. First the standard model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd020e-a5f2-4dfa-b1f1-ae907c703bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "LR_model = LogisticRegression(penalty=None, max_iter=10000)\n",
    "\n",
    "# Fit the model\n",
    "LR_model = LR_model.fit(X_train, Y_train)\n",
    "\n",
    "# Assess the fit\n",
    "y_train_pred = LR_model.predict(X_train)\n",
    "y_test_pred = LR_model.predict(X_test)\n",
    "print(\"Regular Logistic Regression\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Accuracy on training data = {accuracy_score(Y_train, y_train_pred):.3f}\")\n",
    "print(f\"Accuracy on test data = {accuracy_score(Y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecdeeb2-56cf-4422-bd6c-51e318fa066d",
   "metadata": {},
   "source": [
    "We can do the same thing, using Ridge regularisation, simply by removing \"penalty=None\" from the call to `LogsiticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde2ef1-e2a3-491c-a70b-2d35c46da9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "Ridge_LR = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# fit the model\n",
    "Ridge_LR = Ridge_LR.fit(X_train, Y_train)\n",
    "\n",
    "# assess the fit\n",
    "print(\"Ridge Regularised Logistic Regression\")\n",
    "print(\"-------------------------------------\")\n",
    "y_train_pred = Ridge_LR.predict(X_train)\n",
    "y_test_pred = Ridge_LR.predict(X_test)\n",
    "print(f\"Accuracy on training data = {accuracy_score(Y_train, y_train_pred):.3f}\")\n",
    "print(f\"Accuracy on test data = {accuracy_score(Y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7281e-0bcb-4b88-8bad-ae02d8a2f62b",
   "metadata": {},
   "source": [
    "Note that the accuracy on the training data is worse for the regularised model than it is for the plain LR model, but we get a small improvement in performance on the test data. \n",
    "\n",
    "At the moment we are putting equal weight on how well the model fits the training data, and how big the values of the $\\beta$s are. The parameter C dictates how the model should balance being accurate on the training data vs keeping the values of the $\\beta$s small. The smaller the value of C, the more attention the model pays to the size of $\\beta$, and the smaller their values will be forced to be. By default, the value of C is 1. \n",
    "\n",
    "C is known as a \"hyperparameter\" - it dictates how the model works, rather than encapsulating something about the data itself. \n",
    "\n",
    "How do you pick good values for hyperparameters? \n",
    "\n",
    "You use Cross-Validation to try out several different values for your hyperparameters, and pick the one that works the best!\n",
    "\n",
    "Luckily for us, `scikit-learn` has a function that does this built into it (because of course it does). The model `LogisticRegressionCV` will take a list of different hyperparameter values, try them all using cross-validation, and pick the ones that work the best. In the case of the C hyperparameter, we pass a list of different values we would like it to try (we also need to tell it what values of the `l1_ratio` hyperparameter to use, but don't worry about that for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fddabc7-d6ab-4d63-9183-6d0e826e05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "RidgeCV = LogisticRegressionCV(Cs=[1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "                               random_state=2, max_iter=10000, l1_ratios=(0.0,),\n",
    "                               use_legacy_attributes=False)\n",
    "\n",
    "# Fit the model\n",
    "RidgeCV = RidgeCV.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546a45b-c247-4d5e-93f2-49317f7735d6",
   "metadata": {},
   "source": [
    "By default `LogisticRegressionCV` does 5-fold cross-validation. That means that for each of the Cs we tested, there are 5 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b374a71-efd6-4a23-85c7-0b141e8d1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeCV.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40743428-b668-4d25-8b44-b7fe793847c4",
   "metadata": {},
   "source": [
    "We can plot each one against the value of C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff05432-c3e9-448c-8be8-46679bdb72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(RidgeCV.Cs_, RidgeCV.scores_[:,0,:].T, \"ko\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel( \"log (C)\" )\n",
    "plt.ylabel( \"accuracy score\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358b27a-f6bb-42c1-b06f-aab1cdfe6bad",
   "metadata": {},
   "source": [
    "This might be easier if we looked at the mean score for each value of C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017dcab-d0c0-4794-876b-47716ce37dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = RidgeCV.scores_[:,0,:].mean(axis=0)\n",
    "plt.plot(RidgeCV.Cs_, mean_acc, \"ko-\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel( \"log (C)\" )\n",
    "plt.ylabel( \"accuracy score\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6489c-06cb-43a0-b805-e234480a3e89",
   "metadata": {},
   "source": [
    "We can see that the best mean score here is for $C=10^{-1}$ or 0.1. Indeed, if we ask RidgeCV what it is using, then it will tell us it has picked this as the best value for C. \n",
    "\n",
    "We can now have a look at the accuracy on the test and training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a004a1b-7cdf-4d33-bfd8-c5c68043b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the accuracy.\n",
    "y_train_pred = RidgeCV.predict(X_train)\n",
    "y_test_pred = RidgeCV.predict(X_test)\n",
    "print(\"Ridge Regularised Logistic Regression Hyperparamter search\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Optimal parameter: C={RidgeCV.C_}\")\n",
    "print(f\"Accuracy on training data = {accuracy_score(Y_train, y_train_pred):.3f}\")\n",
    "print(f\"Accuracy on test data = {accuracy_score(Y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480c57a-8db6-4b56-b9db-ebdeee7c48a1",
   "metadata": {},
   "source": [
    "We can see that now the model is just as accuracy on the test data as it is on the training data (perhaps even a little more, although this is probably just random noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8ced5-2bc0-49f6-a743-aa48e4e4d72d",
   "metadata": {},
   "source": [
    "## L1 vs L2 regularisation, and grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66bf3a-25ad-4f31-9b6e-f5b54d6d810a",
   "metadata": {},
   "source": [
    "You'll have noticed that in the call to `LogisticRegressionCV` we give two lists of paramteres - `Cs` and `l1_ratios`. We've talked about what `Cs` is, but what about `l1_ratios`.\n",
    "\n",
    "The type of regularization we've reffered to as \"Ridge\" is also known as L2 regularisation. This is because the penalty term is $\\frac{1}{C}\\sum_i{\\beta^2}$. There is another sort of regularisation called \"Lasso\" regularisation, or L1 regularization. Here the penalty term is $\\frac{1}{C}\\sum_i{|\\beta|}$. So the full loss term is:\n",
    "\n",
    "$$ Loss = L(y,\\bar{y}) + \\frac{1}{C}\\sum_i{|\\beta|} $$\n",
    "\n",
    "We can specify to use L1, rather than L2 regression by setting the `l1_ratio=1` in our calls to `LogisticRegression`. Why might you use one vs the other? In short Lasso favours setting as many $\\beta$ values as possible to 0 - that is, it tries to not use some variables at all, if it can avoid it, where as in Ridge, where as Ridge uses all the values, but keeps them low. \n",
    "\n",
    "So, in Lasso $\\beta_1=0, \\beta_2=0.9$ is better than $\\beta_1=0.5, \\beta_2=0.5$ because $0 + 0.9 < 0.5 + 0.5$. However, the opposite is true in Ridge, because $0.9^2 > 0.5^2 + 0.5^2$. \n",
    "\n",
    "This means that if you have a set of variables/features that basically contain the same information, Lasso will pick one of them to use and discard the rest, while Ridge will take a kind of average. Ridge tends (but not always) gives a more accurate answer, while the answer from Lasso tends to be more interpretable. \n",
    "\n",
    "### ElastiNet\n",
    "\n",
    "Finally, ElastiNet regression uses a mix of Ridge and Lasso, and this is where the `l1_ratio` comes in. It says how to balance L1 vs L2 regulariastion. The full equation is:\n",
    "\n",
    "$$ Loss = L(y, \\bar{y}) + \\frac{l1_ratio}{C}\\sum_i{|\\beta|} + \\frac{(1-l1_ratio}{C}\\sum_i{\\beta^2} $$\n",
    "\n",
    "when `l1_ratio=0` then this is the same as Ridge regression. When `l1_ratio=1` its the same as Lasso. And when `0 < l1_ratio < 1` then you have an intermediate postion. \n",
    "\n",
    "Again, we can use cross-validated search to find the best value for `l1_ratio`. \n",
    "\n",
    "If we are also look for `C` we can do a `grid search` that is take, say 5 values for `C`, and 5 values for `l1_ratio` and try all 25 combinations to see which is best. `LogisticRegression` will allow us to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b5263-3797-41ac-8243-545324e6f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElastiNetCV = LogisticRegressionCV(Cs=[1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "                                   l1_ratios=[0.0, 0.5, 1.0],\n",
    "                                   max_iter=10000,\n",
    "                                   random_state=2,\n",
    "                                   use_legacy_attributes=False,\n",
    "                                   # We have to use the \"saga\" solver as the standard algo can't do l1_ratio\n",
    "                                   solver=\"saga\")\n",
    "ElastiNetCV = ElastiNetCV.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3856ac5-d372-484d-b167-666a64864b24",
   "metadata": {},
   "source": [
    "Lets have a look at the resulting scores. Notice that what we end up with is 5 matricies, one for each of the 5 folds in the cross validation. Each matrix is 3 rows by 9 columns. Each of the rows corresponses to a value of `l1_ratio` and each column to a value of C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f846ad-b899-48ed-968d-c2d92583e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElastiNetCV.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea129b3-e7f7-40eb-be4b-23710a6821e6",
   "metadata": {},
   "source": [
    "This is far to complex to look at by eye, so lets take some averages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275392eb-c47f-49c6-8577-f2598199e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElastiNetCV.scores_.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b03003-7f41-4563-8e3a-95c88b7f635d",
   "metadata": {},
   "source": [
    "We can now plot these and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500f6b8-03f7-4d49-a675-5e557f86c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ElastiNetCV.Cs_, ElastiNetCV.scores_.mean(axis=0).T, \"o-\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend(ElastiNetCV.l1_ratios, title=\"l1_ratio\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2282423-c931-4c47-ae53-e3811e75db86",
   "metadata": {},
   "source": [
    "The blue line above is identical to the hyperparameter search we did with the Ridge regression. The green line, which is lasso regression is always less accurate than the blue line. However, the yellow line, which uses 50% ridge and 50% lasso does just about get better than the pure Ridge, for C=0.1 only. Thus, the most accurate hyperparamteres on the cross-validations are C=0.1, l1_ratio=0.5. Lets see how that does on the full training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5f393-da7e-4caa-8e25-5a6ee18d2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = ElastiNetCV.predict(X_train)\n",
    "y_test_pred = ElastiNetCV.predict(X_test)\n",
    "\n",
    "print(\"ElastiNet Logistic Regression Hyperparamter search\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(f\"Optimal parameteres: C={ElastiNetCV.C_}, l1_ratio={ElastiNetCV.l1_ratio_}\")\n",
    "print(f\"Accuracy on training data = {accuracy_score(Y_train, y_train_pred):.3f}\")\n",
    "print(f\"Accuracy on test data = {accuracy_score(Y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7e9f0-2a60-4725-9212-947ea9418483",
   "metadata": {},
   "source": [
    "We notice something intersting here - the overall performance on the test data is actually lower here than it was on the Ridge regression. How come? Well, because we are doing everything with randomly selected test, training and cross-validation, and everything is going to be do do with how that spliting works. Perhaps the fact that `l1_ratio=0.5` out performed `l1_ratio=0.0` is simply chance, just the luck of the draw on the cross-validation sets, or on the test set. If we did many test/train splits, we'd get slightly different answers each time, particularly as we are working with quite small training sets. \n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook we've explored two ideas - Regularisation and Hyperparamter search. These are both ideas that apply to more complex models. The more complex a model, generally the more hyperparameters it has. Here we just had two paramteres to play with, but other maight have many more. Regularisation might happen differently in other model types. For example, in RandomForest, you might just exclude a random fraction of predictive features each time you \"grow a tree\" (that fraction being a hyperparameter). \n",
    "\n",
    "You can now try to regularise your models. \n",
    "\n",
    "Next time we'll take a look at creating RandomForest models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIS312-ML",
   "language": "python",
   "name": "bis312-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
