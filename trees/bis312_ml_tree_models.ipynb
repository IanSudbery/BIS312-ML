{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38ac7e1-a047-4a5e-9378-bcc94f9915da",
   "metadata": {},
   "source": [
    "# Tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ee24d-e0cf-459d-9578-1dba5938f1ab",
   "metadata": {},
   "source": [
    "So far all the models we have looked at have been variations of a linear model. Specifically they have mostly been variations of a logistic regression model, but the theory is very similar for using general linear models for predicting continuous values. \n",
    "\n",
    "Linear models are great - they have well established methods for finding the best paratemeter values and its really easy to see from the results which are the variables that are contributing to the prediction. With methods like Lasso, Ridge and ElastiNet, we also have ways of trying to combat overfitting. But they suffer from a major problem - they can only find linear decision boundaries! \n",
    "\n",
    "What do I mean by that? Well, consider a logistic regression where $\\beta_1=0.5, \\beta_2=0.25$, \n",
    "\n",
    "so that $z=0.5 \\times x_1 + 0.25 \\times x_2$ and $y=\\frac{1}{1 + e^{-z}}$. \n",
    "\n",
    "Here $x_1$ might be tumour volume and $x_2$ tumour curviture, and y might be benin or malignant. \n",
    "\n",
    "The plot below shows this, where the colour corresponds to the value of y for that appropriate values of $x_1$ and $x_2$. Red and Blue points show the true values of the points we are attempting to classify:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cee4e3-335b-4574-8837-c8b3c27df364",
   "metadata": {},
   "source": [
    "![](linear_data_linear_fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34986ea8-6c9d-47e9-9337-7f1384de6a09",
   "metadata": {},
   "source": [
    "You can see a straight line where the prediction goes from blue (y=0) to red (y=1). This is the decision boundary. But what if the decision boundary is not straight? What if it looks like:\n",
    "\n",
    "![](curved_data_linear_boundary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbebb74-0f79-444b-8e02-e0fcc3803667",
   "metadata": {},
   "source": [
    "Here a logistic model does very badly at classifying things as benign or malignant, because it can only use straight lines! This leads to a large collection of red points falling on the blue side of the line. \n",
    "\n",
    "There are many different ways of dealing with this, but one very powerful, popular, and cruitially easy way to do it is with decision tree-based methods.\n",
    "\n",
    "Tree models not only solve the problem of non-linear decision boundaries, but are also resistant to over-fitting, and can handle missing values without you having to do any imputation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d518380-3398-4563-b436-aeb99e1a90f6",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b9b7f-0875-4a7c-a360-2a629d67839c",
   "metadata": {},
   "source": [
    "A decision tree is basically a flow chart that says how to classify something. For example a decision tree might look like:\n",
    "\n",
    "    if x[\"number_of_legs\"] > 2:\n",
    "        if x[\"has_wiskers\"]:\n",
    "           y = \"cat\"\n",
    "        else\n",
    "           y = \"dog\"\n",
    "    else:\n",
    "        if x[\"is_black\"]:\n",
    "           y = \"crow\"\n",
    "        else:\n",
    "           y = \"robin\"\n",
    "\n",
    "With enough levels in your tree you can build an approximation of the curved boundary above. You can also see how the meaning of one feature can be made to be dependent on the meaning of another (which is really the same thing as a decision boundary that isn't a straight line).\n",
    "\n",
    "We'll look at two tree models: Random Forests and extreme gradient boosted trees (XGBoost).\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "In a random forest you take a subset of your features, and a subset of your training examples, and make the best tree you can out of them. You then repeat this for many different subsets of training examples and features (hence a forest). When you classify an example, each tree gets a vote, and then class with the most votes wins. \n",
    "\n",
    "Training a random forest is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5d9828-e3c2-472e-be91-2fb74dc820b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0, F1: 1.000\n",
      "Test accuracy: 0.956, F1: 0.966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, Y = breast_cancer[\"data\"], breast_cancer[\"target\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "RF_model = RandomForestClassifier(random_state=1)\n",
    "RF_model = RF_model.fit(X_train, Y_train)\n",
    "\n",
    "y_train_pred = RF_model.predict(X_train)\n",
    "y_test_pred = RF_model.predict(X_test)\n",
    "\n",
    "print(f\"Training accuracy: {accuracy_score(Y_train, y_train_pred)}, F1: {f1_score(Y_train, y_train_pred):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(Y_test, y_test_pred):.3f}, F1: {f1_score(Y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c890b7-d191-47d5-b01a-183aa8d716c7",
   "metadata": {},
   "source": [
    "So with no tuning at all, we have already gotten perfect scores on the training data, and 95.6% accuracy on the test data. \n",
    "\n",
    "Like Lasso and Ridge Regression, there are parameters that need to be tuned for a RandomForest. In fact, Random Forests have many paratameters to tune, but two are particularly important. The first is `n_estimators` and the second is `max_features`. \n",
    "\n",
    "`n_estimators` says how many trees to use. Generally the more the better, but the slower and more memory intensive your model will be. Its also possible that you might suffer from over-fitting if you \"grow\" too many trees. \n",
    "\n",
    "`max_features` says how many features each tree should use. By default this is $\\sqrt{total features} \\approx 5$ in the case of the breast cancer data set. The more features that are used, the higher the accuracy, but the larger the probabilit of over-fitting. \n",
    "\n",
    "Again, we can use cross validation to help us pick the best values. \n",
    "\n",
    "Unlike `LogisticRegression`, there is no `RandomForestCV`. However, `sklearn` does provide the general purpose `GridSearchCV` and `RandomSearchCV`, which can be used with any model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d84e07a-3216-480a-bad1-dd982e18ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "RF_model = RandomForestClassifier(random_state=1)\n",
    "params = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 400],\n",
    "    \"max_features\": [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(RF_model, params, return_train_score=True)\n",
    "rf_cv = rf_cv.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de370b-d5aa-43bb-ae22-0d2172351e73",
   "metadata": {},
   "source": [
    "The `cv_results_` attribute of the gridsearch contains the results for every combination of paramater vaules, but it is a very massive table, with too much data. We can make it more understandable, by just selecting a few columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4abdbc-ab86-473b-93d6-830d83bf0733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.998901</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.10</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.998352</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.20</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.20</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.20</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.20</td>\n",
       "      <td>400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.30</td>\n",
       "      <td>50</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.936264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.30</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.30</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.30</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.30</td>\n",
       "      <td>400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.40</td>\n",
       "      <td>50</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.40</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.40</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.40</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.40</td>\n",
       "      <td>400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.998352</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.50</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.50</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.50</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.50</td>\n",
       "      <td>400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_max_features  param_n_estimators  mean_train_score  mean_test_score\n",
       "0                 0.05                  50          0.999451         0.942857\n",
       "1                 0.05                 100          1.000000         0.947253\n",
       "2                 0.05                 150          1.000000         0.945055\n",
       "3                 0.05                 200          1.000000         0.947253\n",
       "4                 0.05                 400          1.000000         0.953846\n",
       "5                 0.10                  50          0.998901         0.945055\n",
       "6                 0.10                 100          1.000000         0.947253\n",
       "7                 0.10                 150          1.000000         0.956044\n",
       "8                 0.10                 200          1.000000         0.951648\n",
       "9                 0.10                 400          1.000000         0.951648\n",
       "10                0.20                  50          0.998352         0.947253\n",
       "11                0.20                 100          1.000000         0.949451\n",
       "12                0.20                 150          1.000000         0.951648\n",
       "13                0.20                 200          1.000000         0.949451\n",
       "14                0.20                 400          1.000000         0.949451\n",
       "15                0.30                  50          0.997802         0.936264\n",
       "16                0.30                 100          1.000000         0.936264\n",
       "17                0.30                 150          1.000000         0.942857\n",
       "18                0.30                 200          1.000000         0.945055\n",
       "19                0.30                 400          1.000000         0.942857\n",
       "20                0.40                  50          0.997802         0.945055\n",
       "21                0.40                 100          1.000000         0.940659\n",
       "22                0.40                 150          1.000000         0.936264\n",
       "23                0.40                 200          1.000000         0.942857\n",
       "24                0.40                 400          1.000000         0.942857\n",
       "25                0.50                  50          0.998352         0.947253\n",
       "26                0.50                 100          1.000000         0.947253\n",
       "27                0.50                 150          1.000000         0.949451\n",
       "28                0.50                 200          1.000000         0.945055\n",
       "29                0.50                 400          1.000000         0.947253"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rf_cv.cv_results_)[[\"param_max_features\",\n",
    "                                 \"param_n_estimators\",\n",
    "                                 \"mean_train_score\",\n",
    "                                 \"mean_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf2241-6c3d-4eb2-a19f-84ebcf5f6de0",
   "metadata": {},
   "source": [
    "You'll note that in general the more estimators the better, and actaully in this case, the feature features used per tree the better! We can look at what the best parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d47de5-62ac-4aa3-b4f1-12ee25726f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 0.1, 'n_estimators': 150}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2660dfa9-2993-43d1-8718-5d2b1f440363",
   "metadata": {},
   "source": [
    "The absolute best score doesn't come from the most estimators, but if you look back at the table, you'll see that scores for max_features = 0.1 for 200 and 400 estimators are not much worse, so its probably just random noise. Still, using a smaller number of trees saves us time, and I think we can at least say that using more doesn't give us a better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e14feb-9df5-43cf-9a56-4dd0f7f1587c",
   "metadata": {},
   "source": [
    "### (eXtreme) Gradient Boosting Trees\n",
    "\n",
    "An alternative type of tree model to RandomForests is a gradient boosting tree. In a gradient boosting tree model, instead of learning a bunch of random trees, a single tree is learnt, and the difference between the predictions and the truth calculated (the loss, see last week). A second tree is then learnt specifically to correct the error of the first. A third tree is then learnt to correct the errors of the second, and so on...\n",
    "\n",
    "A very popular implementation of boosting trees is eXtreme gradient boosting trees (XGBoost). This adds in regularisation (like the Ridge and Lasso regularisation we saw last week), and a whole bunch of computer science tricks to make the thing run efficiently on large datasets. \n",
    "\n",
    "XGBoost is pretty much the forfront of models designed to work with what is called \"structured\" data - that is data that comes in tables, rather than things like images, free text or sound files. \n",
    "\n",
    "However, thanks to sklearn, using it is not much more difficult than any other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c93269f-02d2-4076-b350-281e1701767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0, F1: 1.000\n",
      "Test accuracy: 0.956, F1: 0.966\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "y_train_pred1 = xgb_model.predict(X_train)\n",
    "y_test_pred1 = xgb_model.predict(X_test)\n",
    "\n",
    "print(f\"Training accuracy: {accuracy_score(Y_train, y_train_pred1)}, F1: {f1_score(Y_train, y_train_pred1):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(Y_test, y_test_pred1):.3f}, F1: {f1_score(Y_test, y_test_pred1):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1abfc-3eb1-4794-ac63-0fd30f8acfe3",
   "metadata": {},
   "source": [
    "XGBoost has [pages and pages](https://xgboost.readthedocs.io/en/release_3.2.0/parameter.html#parameters-for-tree-booster) of parameters that you can use for tuning. The manual recommends having a look at `n_estimators`, `max_depth`, `min_child_weight`, `gamma` and `eta`. \n",
    "\n",
    "One interesting thing you can do with XGBoost is called \"early stopping\". Instead of providing it with a number of rounds to run for, you instead provide it with your test data, and it just keeps training on the training data until the performance on the test data stops improving. Interstingly, this will stop if your test data performances starts getting worse (i.e. you start overfitting). This is probably best if you have a validation set seperate from your test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad15e98c-29df-4f3e-90a6-564ab6926e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.45819\n",
      "[1]\tvalidation_0-logloss:0.36825\n",
      "[2]\tvalidation_0-logloss:0.29957\n",
      "[3]\tvalidation_0-logloss:0.26435\n",
      "[4]\tvalidation_0-logloss:0.23181\n",
      "[5]\tvalidation_0-logloss:0.20999\n",
      "[6]\tvalidation_0-logloss:0.19081\n",
      "[7]\tvalidation_0-logloss:0.17827\n",
      "[8]\tvalidation_0-logloss:0.16918\n",
      "[9]\tvalidation_0-logloss:0.16826\n",
      "[10]\tvalidation_0-logloss:0.16249\n",
      "[11]\tvalidation_0-logloss:0.15697\n",
      "[12]\tvalidation_0-logloss:0.15583\n",
      "[13]\tvalidation_0-logloss:0.15824\n",
      "[14]\tvalidation_0-logloss:0.16045\n",
      "[15]\tvalidation_0-logloss:0.15394\n",
      "[16]\tvalidation_0-logloss:0.15391\n",
      "[17]\tvalidation_0-logloss:0.15644\n",
      "[18]\tvalidation_0-logloss:0.15714\n",
      "[19]\tvalidation_0-logloss:0.15831\n",
      "[20]\tvalidation_0-logloss:0.15838\n",
      "[21]\tvalidation_0-logloss:0.15724\n",
      "[22]\tvalidation_0-logloss:0.15654\n",
      "[23]\tvalidation_0-logloss:0.15829\n",
      "[24]\tvalidation_0-logloss:0.16084\n",
      "[25]\tvalidation_0-logloss:0.15986\n",
      "[26]\tvalidation_0-logloss:0.16073\n",
      "Training accuracy: 0.9978021978021978, F1: 0.998\n",
      "Test accuracy: 0.956, F1: 0.966\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier( early_stopping_rounds=10)\n",
    "xgb_model.fit(X_train, Y_train, eval_set=[(X_test, Y_test)])\n",
    "\n",
    "y_train_pred1 = xgb_model.predict(X_train)\n",
    "y_test_pred1 = xgb_model.predict(X_test)\n",
    "print(f\"Training accuracy: {accuracy_score(Y_train, y_train_pred1)}, F1: {f1_score(Y_train, y_train_pred1):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(Y_test, y_test_pred1):.3f}, F1: {f1_score(Y_test, y_test_pred1):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe72aed-d565-41e6-bd7d-29f392fbba2d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have now covered how to use LogsiticRegression, Ridge, Lasso, ElastiNet, RandomForest and XGBoost. We've covered the ideas of test, train and validation sets, and cross-validation. We've talked about data preprocessing, hyperparameter tuning and grid search. You are now pretty much ready to go out there and build some models!\n",
    "\n",
    "The final thing to talk about is model interpretation, and that will come next week.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIS312 ML",
   "language": "python",
   "name": "bis312-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
