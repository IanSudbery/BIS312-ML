{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c783c9a-cbbc-4bc6-b309-548bcc49d9db",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning introduction: practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6458dad-b6ae-4091-bc13-940fcd66565b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The basic machine learning project consists of the same steps:\n",
    "\n",
    "1. Aquire data\n",
    "2. Split data into Train/Test and (optionally) validation sets\n",
    "3. Preprocess the data\n",
    "4. Define the model\n",
    "5. Fit the model to the training data\n",
    "6. Measure the accuracy of the model on the training data\n",
    "7. Measure the accuracy of the model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc15d24-d9e1-4c4a-9ede-145d48a7c72f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "You might go through several iternations of 3-6 while you get your model working, and that is where validation sets are useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6434cb3-d1cb-47aa-b838-0e082c1dc7f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Their are many different programming frameworks for doing machine learning. Many of them are written in C and then export \"language bindings\" to allow them to be used from python, R, matlab, java, Ruby, Scala...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a04a7-be3c-4a49-9fb0-83c94d0cdf66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Some examples are:\n",
    "\n",
    "* TensorFlow\n",
    "* Keras\n",
    "* PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9c9a7-3169-4432-a334-fd06efb074ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "These are often optimised to allow very powerful \"Deep Learning\" neural network models, such as CNNs, ResNets, LSTMs and Transformers. \n",
    "\n",
    "These are trained on thounssands of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffad7c-e742-411c-a2f2-fee4ff6f2626",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We will be using `scikit-learn`\n",
    "\n",
    "* Focuses more on \"traditional\" learning algorithms\n",
    "* Extremely easy to use\n",
    "* Runs well on a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "41e94c58-d02a-423b-bfb6-b70d923bc2d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2fbd8-47e9-42d7-8b79-576920c4aae1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Aquire the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158e45f6-c879-4b25-aa93-f3a6706add00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For this example we will be using an example dataset included with `scikit-learn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4d6f0-3c90-4460-b84a-2db8221a3b22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This dataset contains clinical data from patients with breast cancer, and whether their tumour is benign or malignant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f57ebc92-1961-46dc-9503-13273adc1328",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8681d-1b88-41a5-9563-44e122c68487",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "If you are interested, the data includes a description of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f7e06283-83ac-4ab4-a8b9-f6b75ec38fc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer Wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9c841-5f32-44e5-a151-2bb4ad782569",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Traditionally in machine learning we use the variable X to represent the features of the training data, and Y to represent the thing we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8c1c8a3b-e345-4074-807b-d0a10f1ddd56",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = breast_cancer.data\n",
    "Y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fa0c2-82b5-4a1d-8f0c-84c1da567c42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Lets have a look inside these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4b4c3c3-acde-4e0b-894c-21a96894b3f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "05a6f966-276e-44ed-bc46-06bfdfd9237a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feae94a-47b5-466e-b6fa-7db3778daf6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Lets investigate these datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739c222-c983-4063-91a7-036f8ed6082a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "X and Y are `array`s. Arrays are like lists, except: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94664411-f978-4205-93f6-a6e965561f51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* They can have more than one dimension (e.g. rows AND columns).\n",
    "* They can only have one data type in them.\n",
    "* You can't `append` or `extend` them. You have to predefine their size.\n",
    "\n",
    "(most things in `sklearn` can also be done with pandas dataframes). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec6354-5774-40be-92d9-236ec7598d4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The `shape` attribute allows us to see how big they are. It returns an array, the first entry\n",
    "* the first entry contains the number of rows\n",
    "* the second entry contains the number of columns (if it has them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd7d5667-1213-41c9-b930-1d7922ab65ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples is 569\n",
      "Each 30 features\n",
      "They predict 569 class labels \n"
     ]
    }
   ],
   "source": [
    "n_x = X.shape[0] \n",
    "n_features = X.shape[1]\n",
    "n_y = Y.shape[0]\n",
    "\n",
    "print(\"The number of examples is %i\" % n_x)\n",
    "print(\"Each has %i features\" % n_features)\n",
    "print(\"They predict %i class labels \" % n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e134c-bd46-415e-88d8-b44fcb9f3b24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "You'll have seen that Y contains 1s and 0s. Remember that the thing we are trying to predict has to be numeric. \n",
    "\n",
    "But what does 0 and 1 mean? \n",
    "\n",
    "We can find out from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "94bd5026-308d-4e87-a417-3b172656dd0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = breast_cancer[\"target_names\"]\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24ad9f-b1b0-4720-a4b8-78e545dc9e0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "So:\n",
    "\n",
    "* when y is 0, the tumour is malignant\n",
    "* when y is 1, the tumour is benign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba791be-06f4-4b09-b9ae-efbd61d28db8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can look which is which using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3c838fc0-2abc-444d-af72-cf3f082a645b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'malignant', 'malignant', 'benign', 'benign',\n",
       "       'benign', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'benign', 'malignant', 'malignant', 'malignant',\n",
       "       'malignant', 'malignant', 'malignant', 'malignant', 'malignant',\n",
       "       'benign', 'malignant', 'benign', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[Y][1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23467d-257d-4447-a1b5-230c055319d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can also have a look at what each of the features is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "48917e70-2f4f-4a3e-90df-33df85afe573",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c298e-4214-4653-92f5-00e98ad7a34b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae83b91-9f26-47f3-93d0-e12f81f86283",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Now that we have got our data, and understood it, the next step is to split into test and train data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43df0a-cfc0-4138-9ad5-300e421bf5cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This is important to ensure that we don't \"overfit\" on the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992b699-41fe-484c-8ba7-4a6b727c21b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "That is - learn how to distinguish these examples very well but only these examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8514c81b-1ef1-49c3-8c03-ef79acc91cc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the full data is 569\n",
      "The number of training examples is 398\n",
      "The number of testing examples is 171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn provides the `train_test_split function to do the splitting for you\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, shuffle = True, random_state=321)\n",
    "\n",
    "# Check the split has worked. \n",
    "n_x_train = X_train.shape[0]\n",
    "n_x_trest = X_train.shape[0]\n",
    "\n",
    "print (\"The number of examples in the full data is %i\" % n_x)\n",
    "print (\"The number of training examples is %i\" % X_train.shape[0])\n",
    "print (\"The number of testing examples is %i\" % X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc9657-e13e-4e57-b2f8-529e38de5781",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The amount of data taken for testing depends on the problem. \n",
    "\n",
    "You need enough to get a good estimate of how well you are doing.\n",
    "\n",
    "But not so much you arn't left with enoguh for training. \n",
    "\n",
    "10%-30% is traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812f2ca-ec12-4f01-ad44-96e1a5fa5673",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "There are other ways to do the splitting\n",
    "\n",
    "Noteably:\n",
    "\n",
    "* Creating a validation set as well as a test set\n",
    "* Cross-validation (see below).\n",
    "\n",
    "But also if you need to make sure certain examples do or do not end up together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebcf8c-d72e-44a9-9170-94911a0a5ff3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d6194-af72-4cb8-9655-fda5f2cfe10e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We are going to come back to this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e7c37-95e2-4be9-bc97-9dbe50db7e61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd942207-9a87-4296-a8ea-bf06962b3623",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this example we will use perhaps the most simple classification model:\n",
    "\n",
    "Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623a554-b9e8-4286-8c22-79282cff3b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "In logistic regression we first define a score for each example by multiplying each feature by a weight and additing it together:\n",
    "\n",
    "$$ Z = \\beta_1 \\times x_1 + \\beta_2 \\times x_2 + ... + \\beta_{30} \\times x_{30} $$\n",
    "\n",
    "Here $x_n$ is a feature. So for example, $x_1$ is \"mean radius\". $\\beta_1$ is some score we associate with the first feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89605435-1c95-40b1-807c-0a2190feb8d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This is often written \n",
    "$$ Z = \\sum_n \\beta_n \\times x_n $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ Z = \\beta^T X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24ef60-b995-4340-8cfd-f15e0b1af1c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Z is then transformed using the \"logistic function\":\n",
    "\n",
    "$$ Y = \\frac{1}{1 + e^{-Z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdeac81-149b-457c-9673-6fe7ff140668",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The job of ML here is to find the values of $\\beta$ that get Y closest to the real values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621e8d1-e54a-4979-b19d-5e2a4d1e8e59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can create a LogisticRegression model using `LogisticRegression` from `sklearn`.\n",
    "\n",
    "There are many options, some of which you will use later.\n",
    "\n",
    "For now we'll (mostly) use the defaults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dec3fb25-a174-4b1c-8a44-392880473ba9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_model = LogisticRegression(penalty=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47381ec-0c0c-497c-b7be-15c2877316d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da933bf-e9be-497c-81dd-199db4651275",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Fitting the model is the easiest bit!\n",
    "\n",
    "We just provide `X_train` and `Y_train` to our model's `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ad7bbe92-edd1-4505-bdb3-070aa27631db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_fit = LR_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e255bf-62ad-4e72-b33c-75600a61e1df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "There are two warnings.\n",
    "\n",
    "One tells us that we are calling the function wrong.\n",
    "\n",
    "The other that our model hasn't converged. This might be a problem, but we'll come back to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76183ce4-9fca-4295-925c-fae401c6b5d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Measure the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f67d45-1655-4f79-bd29-27f0765c4f37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The next step is to measure the accuracy of our model. There are many metrics for measureing how good your model is. \n",
    "\n",
    "The simplest is \"accuracy\" - simply the fraction of guesses that are correct.\n",
    "\n",
    "Lets define an accuracy function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "adf62a4e-ce88-403d-8806-e075d3d302b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    ''' \n",
    "    The accuracy is the number of correct predictions, divided by total number of predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    y_true : numpy array-like, array of true classes for each example, encode is 0 for \n",
    "        False, 1 for true\n",
    "\n",
    "    y_pred : numpy array-like, array of predicted classes for each example, encoded as \n",
    "        above\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    score : float - the accuracy.\n",
    "    '''\n",
    "\n",
    "    # number of examples\n",
    "    total = len(y_true)\n",
    "    \n",
    "    # Calculate the difference between the predictions and the truth (remember benign is 0, malignant is 1)\n",
    "    # first do it per example\n",
    "    wrong = abs(y_true - y_pred)\n",
    "\n",
    "    # then count the number of examples\n",
    "    n_wrong=sum(wrong)\n",
    "\n",
    "    # subtract from the total number to get the number right. \n",
    "    n_right = total - n_wrong\n",
    "\n",
    "    # accuracy is now the ratio of these two\n",
    "    accuracy = float(n_right/total)\n",
    "    \n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe4c15-febb-491d-819f-f9eaf9bb6a0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can now use this to ask how well our trained model predicts the true class labels of our training examples from our training data.\n",
    "First predict the values of Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "748b754d-ff17-4bcb-8391-e4ac730531ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train_pred = LR_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465b5d1-2f8b-43af-ba49-4605846b1f34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Then compare them to the correct values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8572612f-429e-4cd2-a994-04fe03467360",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9597989949748744"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_train, Y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d64b3-efb8-4410-bc79-bbbe921c9a1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "95% accuracy seems pretty good, but is this also true in the test data. \n",
    "\n",
    "Remember overfitting can lead to models working less well on data other than what they were trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ddc11fad-5552-46d1-a159-74f2d1b8d5b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239766081871345"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = LR_model.predict(X_test)\n",
    "accuracy_score(Y_test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5339a-3aa4-4703-8e6e-8a39d9406548",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Accuracy works fairly well when you have an equal number of examples in each class - the same number of benign or malignant turmours. \n",
    "\n",
    "It works badly where this is not the case. \n",
    "\n",
    "Imagine - only 10 out of 300 cases are malignant. \n",
    "\n",
    "A model that just guess all examples were benign would have an accuracy of 290/300 ~ 97%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd0c73-abd9-4c8c-ad1e-f10048f86621",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Alternatives to accuracy include \n",
    "* precision (the number of cases called malignant that are\n",
    "* recall (the number of cases that are malignant that are called malignant)\n",
    "* F1 score - an average of the above.\n",
    "* AUC (Area under the reciever operator curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4e9e4ffa-be73-480d-a224-81a6ff86abdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 score is 0.97\n",
      "Test F1 score is 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "print (\"Training F1 score is %.2f\" % f1_score(Y_train, Y_train_pred))\n",
    "print (\"Test F1 score is %.2f\" % f1_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6f6aeb6a-a182-4e5a-9508-af70a2338d58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC score is 0.96\n",
      "Test AUC score is 0.92\n"
     ]
    }
   ],
   "source": [
    "print (\"Training AUC score is %.2f\" % roc_auc_score(Y_train, Y_train_pred))\n",
    "print (\"Test AUC score is %.2f\" % roc_auc_score(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43e098-17a7-4858-8cc6-0bdda5ac8636",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* So on the test data its 92%, which is less good, but not massively so. \n",
    "\n",
    "* Still our current model is getting nearly 10% of guesses wrong.\n",
    "\n",
    "* This would be no good if we were telling people if their cancer was benign or malignant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26333dc4-2fdd-4698-8389-f5591e7ac008",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3468a-bf04-477a-9d9f-41d56f0d4e95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "There several things we can do to the data or the model to try to improve this performance, before we move to a more complex model. \n",
    "\n",
    "One common thing to do is to do one or more of a common set of transformations to the data. \n",
    "\n",
    "The most common are:\n",
    "\n",
    "* Centering the data\n",
    "* Scaling the data\n",
    "* Taking the log of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914e8a5-7a9c-49aa-a79b-4b2cce2318aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Some times you have principled reasons to believe that something should be done. \n",
    "\n",
    "More often, you just try and see if it makes things better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099d8cc-fcaa-419f-9e4f-e0a860889bc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Lets try centering and scaling the data. This meakes the mean and standard deviation of each of the features the same. \n",
    "\n",
    "To do this, we create a \"StandardScaler\" object, and \"fit\" it to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "57b70070-081e-4acc-90f0-a4551c856a05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0ef24-ed01-403b-8d88-fda8ceb951f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can now use the `transform` method of this object to scale our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a49d552a-d557-4420-aee8-1bc7609cbf09",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7c93f-a8eb-4c11-bb44-72c2e75793ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can now see that the means are all 0. \n",
    "\n",
    "Means before transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac11aa7a-8b45-4053-8890-cf5d912f58bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.39741658e+01, 1.93426382e+01, 9.08990201e+01, 6.40355025e+02,\n",
       "       9.55764824e-02, 1.01927035e-01, 8.52440653e-02, 4.64194673e-02,\n",
       "       1.80827387e-01, 6.26657538e-02, 4.02647236e-01, 1.23964497e+00,\n",
       "       2.86333266e+00, 4.02035854e+01, 7.11084925e-03, 2.50634221e-02,\n",
       "       3.09895525e-02, 1.14432588e-02, 2.05774070e-02, 3.74401080e-03,\n",
       "       1.60196332e+01, 2.56463568e+01, 1.05591834e+02, 8.52779899e+02,\n",
       "       1.30876106e-01, 2.48253719e-01, 2.62752611e-01, 1.09040957e-01,\n",
       "       2.87438693e-01, 8.36328141e-02])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d3bdb7-275a-4613-b717-9724aafe9acb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Means after transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3a957dbc-d9ac-468a-bcf0-74e1e06ace6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.77953802e-15, -6.38238764e-16,  1.81485201e-15,  1.06503304e-15,\n",
       "       -3.39594349e-15, -1.48708516e-15, -3.01824450e-16,  5.83564464e-16,\n",
       "       -3.39378163e-15,  2.89773788e-15, -5.94094846e-16, -7.91382593e-16,\n",
       "        4.90952895e-17, -3.25814194e-16,  2.28181516e-16, -5.69059038e-16,\n",
       "        3.48688136e-16, -1.67593466e-15, -7.52608472e-16,  1.54594372e-15,\n",
       "       -2.12616078e-15,  3.71673658e-15,  1.37076280e-15,  7.92219445e-16,\n",
       "        4.32436052e-15,  6.53302091e-16, -5.24426956e-17,  1.47843770e-16,\n",
       "       -1.74623018e-16, -1.85306823e-15])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca14e6b-f7d4-4781-ba54-8be45346f4f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can now refit our model and see if it does any better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff6c7b4-8497-44a9-b1e4-eac6b3817029",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Before we go any further. We have a problem. \n",
    "\n",
    "We said we didn't want to \"train\" on the \"test\" data so as to keep it seperate, and not overfit. \n",
    "\n",
    "But if we try lots of different modifications to the data, and test on the test data, then we might choose a set of things that are only good for that test data.\n",
    "\n",
    "We will have \"comtaminated\" our test data - used it in developing our model.\n",
    "\n",
    "This is something you should never do. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13e981-e306-4a34-9cc5-2b03f4afbfe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is where **validation** sets come in useful.\n",
    "\n",
    "You effectively have two test sets - one you use in model development, and one you don't. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16b9ee-927c-4ba6-ad4a-280c35bcff08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "However, if we keep breaking our data into smaller and smaller pieces, we'll have none left. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc67b3c-b84f-46e7-90c8-c83934e27a2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A solution to this is Cross-Validation. To use cross validation:\n",
    "\n",
    "1. Break your data into k pieces (say 10 pieces).\n",
    "2. Train on 9, and use the 10th as a validation set.\n",
    "3. Record the performance on your metric of choice.\n",
    "4. Repeat 2-3, but using a different piece as validation.\n",
    "5. Continue until you have used each pieces as the validation set once.\n",
    "6. Take an average of the scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f4816-a746-43c0-a96d-83ac20911518",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "First lets test our unscaled model under cross-validation for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9e26602c-ebc6-4cb5-8a3b-7348cf8fda6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1. 358 training examples (240 positive). 45 validation examples (27 positive). Accuracy 0.97\n",
      "Fold 2. 358 training examples (233 positive). 45 validation examples (27 positive). Accuracy 0.95\n",
      "Fold 3. 358 training examples (232 positive). 45 validation examples (27 positive). Accuracy 0.97\n",
      "Fold 4. 358 training examples (233 positive). 45 validation examples (27 positive). Accuracy 0.93\n",
      "Fold 5. 358 training examples (233 positive). 45 validation examples (27 positive). Accuracy 0.97\n",
      "Fold 6. 358 training examples (234 positive). 45 validation examples (27 positive). Accuracy 1.00\n",
      "Fold 7. 358 training examples (232 positive). 45 validation examples (27 positive). Accuracy 0.95\n",
      "Fold 8. 358 training examples (235 positive). 45 validation examples (27 positive). Accuracy 0.85\n",
      "Fold 9. 359 training examples (234 positive). 45 validation examples (27 positive). Accuracy 0.95\n",
      "Fold 10. 359 training examples (234 positive). 45 validation examples (27 positive). Accuracy 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define how to breakdown the dataset\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=4564)\n",
    "\n",
    "# Create a list to hold the scores. \n",
    "scores = list()\n",
    "i = 0\n",
    "\n",
    "# kfold splits out sets of indicies to use to select data subsets\n",
    "for train_i, test_i in kfold.split(X_train):\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    # Grab the train and test data for this split. \n",
    "    X_train_k, Y_train_k = X_train[train_i], Y_train[train_i]\n",
    "    X_val_k, Y_val_k = X_train[test_i], Y_train[test_i]\n",
    "    \n",
    "    # Fit the model for the training split\n",
    "    LR_model = LogisticRegression(penalty=None)\n",
    "    LR_model = LR_model.fit(X_train_k, Y_train_k)\n",
    "\n",
    "    # Predict the classes for the validation examples. \n",
    "    y_pred = LR_model.predict(X_val_k)\n",
    "\n",
    "    # calculate the score \n",
    "    k_score = accuracy_score(Y_val_k, y_pred)\n",
    "\n",
    "    print (\"Fold %i. %i training examples (%i positive). %i validation examples (%i positive). Accuracy %.2f\" % \n",
    "           (i, X_train_k.shape[0], np.sum(Y_train_k), X_test_k.shape[0], np.sum(Y_test_k), k_score))\n",
    "    scores.append(k_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cd3f00fe-4f6a-4488-8354-f80ac8c9009e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9471794871794872)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a933e5-d6eb-44a6-88bc-55ad05b4b672",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A mean accuracy of 95% is okay. But can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "878cd800-0e93-48d1-9c2b-1569df7fe55d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1. 358 training examples (240 positive). 45 validation examples (27 positive). Accuracy 1.00\n",
      "Fold 2. 358 training examples (233 positive). 45 validation examples (27 positive). Accuracy 0.95\n",
      "Fold 3. 358 training examples (232 positive). 45 validation examples (27 positive). Accuracy 1.00\n",
      "Fold 4. 358 training examples (233 positive). 45 validation examples (27 positive). Accuracy 0.95\n",
      "Fold 5. 358 training examples (233 positive). 45 validation examples (27 positive). Accuracy 0.97\n",
      "Fold 6. 358 training examples (234 positive). 45 validation examples (27 positive). Accuracy 1.00\n",
      "Fold 7. 358 training examples (232 positive). 45 validation examples (27 positive). Accuracy 0.97\n",
      "Fold 8. 358 training examples (235 positive). 45 validation examples (27 positive). Accuracy 0.93\n",
      "Fold 9. 359 training examples (234 positive). 45 validation examples (27 positive). Accuracy 0.95\n",
      "Fold 10. 359 training examples (234 positive). 45 validation examples (27 positive). Accuracy 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/mb1ims/mambaforge/envs/bis312-ml/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=4564)\n",
    "scores = list()\n",
    "i = 0\n",
    "\n",
    "for train_i, test_i in kfold.split(X_train):\n",
    "    \n",
    "    i +=1 \n",
    "\n",
    "    # Grab the train and test data for this split.\n",
    "    X_train_k, Y_train_k = X_train[train_i], Y_train[train_i]\n",
    "    X_val_k, Y_val_k = X_train[test_i], Y_train[test_i]\n",
    "\n",
    "    # transform the training data\n",
    "    scaled_X_train = scaler.transform(X_train_k)\n",
    "    \n",
    "    # Fit the model for the training split\n",
    "    LR_model = LogisticRegression(penalty=None)\n",
    "    scaled_LR_model = LR_model.fit(scaled_X_train, Y_train_k)\n",
    "\n",
    "    # transform the validation data\n",
    "    scaled_X_val = scaler.transform(X_val_k)\n",
    "    \n",
    "    # Predict the classes for the validation examples.\n",
    "    y_pred = scaled_LR_model.predict(scaled_X_val)\n",
    "\n",
    "    # calculate the score \n",
    "    k_score = accuracy_score(Y_val_k, y_pred)\n",
    "\n",
    "    print (\"Fold %i. %i training examples (%i positive). %i validation examples (%i positive). Accuracy %.2f\" % \n",
    "           (i, X_train_k.shape[0], np.sum(Y_train_k), X_test_k.shape[0], np.sum(Y_test_k), k_score))\n",
    "    scores.append(k_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "245db64d-6c9d-4402-a810-baeebe8768c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9672435897435898)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3bbf8-3754-4396-b3f5-c4580a47c08a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A slight improvement to 97%, so a 2% improvement. This might not seem like a lot, but when you are going from 92% -> 95% -> 97%, 2% is nearly half of the possible improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe53302-0e2b-4d03-bd2f-bf220b819f0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You should now have what you need to get started. \n",
    "\n",
    "Go find your data. \n",
    "\n",
    "Load in your data (sklearn will happily take pandas dataframes). \n",
    "\n",
    "Convert the thing you want to predict into numbers. \n",
    "\n",
    "Divide into test-train. \n",
    "\n",
    "Use crossvalidation to test a simply logistic regression model on your data, with and without scaling and centring. \n",
    "\n",
    "Also try with and without log transformation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIS312-ML",
   "language": "python",
   "name": "bis312-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
